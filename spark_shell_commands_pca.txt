import spark.implicits._
import org.apache.spark.SparkConf
import org.apache.spark.ml.clustering.{KMeans, KMeansModel}
import org.apache.spark.ml.feature.{MinMaxScaler, MinMaxScalerModel}
import org.apache.spark.ml.linalg.Vectors
import org.apache.spark.sql.expressions.Window
import org.apache.spark.sql.{DataFrame, Dataset, Row, RowFactory}
import org.apache.spark.sql.functions.{col, collect_list, min, row_number, struct}
import org.apache.spark.ml.regression.LinearRegression
import com.mongodb.spark.MongoSpark
import org.apache.spark.ml.feature.VectorAssembler
import org.apache.spark.ml.linalg.Vector
import org.apache.spark.sql.{Dataset, Row, SparkSession}

var mongoCollection: Dataset[Row] = MongoSpark.load(spark)

val pcaFeatures: Array[String] = Array("mean_sea_level_pressure_pascal",
      "surface_pressure_surface_level_pascal",
      "orography_surface_level_meters",
      "temp_surface_level_kelvin",
      "2_metre_temp_kelvin",
      "2_metre_dewpoint_temp_kelvin",
      "relative_humidity_percent",
      "10_metre_u_wind_component_meters_per_second",
      "10_metre_v_wind_component_meters_per_second",
      "total_precipitation_kg_per_squared_meter",
      "water_convection_precipitation_kg_per_squared_meter",
      "soil_temperature_kelvin",
      "pressure_pascal",
      "visibility_meters",
      "precipitable_water_kg_per_squared_meter",
      "total_cloud_cover_percent",
      "snow_depth_meters",
      "ice_cover_binary")

val assembler: VectorAssembler = new VectorAssembler().setInputCols(pcaFeatures).setOutputCol("features")
val withFeaturesAssembled: Dataset[Row] = assembler.transform(mongoCollection)
val minMaxScaler: MinMaxScaler = new MinMaxScaler().setInputCol("features").setOutputCol("normalized_features")
val minMaxScalerModel: MinMaxScalerModel = minMaxScaler.fit(withFeaturesAssembled)
var normalizedFeatures: Dataset[Row] = minMaxScalerModel.transform(withFeaturesAssembled)
normalizedFeatures = normalizedFeatures.drop("features").withColumnRenamed("normalized_features", "features").select("gis_join", "features")
val pca: PCAModel = new PCA().setInputCol("features").setOutputCol("pcaFeatures").setK(pcaFeatures.length).fit(normalizedFeatures)


+--------+-------------------+--------+-------------------------+
|gis_join|year_month_day_hour|timestep|temp_surface_level_kelvin|
+--------+-------------------+--------+-------------------------+
|G1200870|         2010011000|       0|       297.02488708496094|
|G1200870|         2010011000|       0|       280.64988708496094|
|G1200870|         2010011000|       0|       287.02488708496094|
|G1200870|         2010011000|       0|       279.02488708496094|
|G1200870|         2010011000|       0|       296.14988708496094|
|G1200870|         2010011000|       0|       279.02488708496094|
|G1200870|         2010011000|       0|       291.77488708496094|
|G1200870|         2010011000|       0|       283.64988708496094|
|G1200870|         2010011000|       0|       279.02488708496094|
|G1200870|         2010011000|       0|       286.77488708496094|
+--------+-------------------+--------+-------------------------+
mongoCollection = mongoCollection.select("gis_join", "year_month_day_hour", "timestep", "temp_surface_level_kelvin").na.drop()


+--------+-------------------------+
|gis_join|temp_surface_level_kelvin|
+--------+-------------------------+
|G4804230|        281.4640808105469|
|G5600390|        265.2140808105469|
|G1701150|        265.7140808105469|
|G0601030|        282.9640808105469|
|G3701230|        279.2140808105469|
|G3700690|        280.8390808105469|
|G3701070|        280.9640808105469|
|G4803630|        275.7140808105469|
|G5108200|        273.4640808105469|
|G4801170|        269.3390808105469|
+--------+-------------------------+
val clusteringCollection: Dataset[Row] = mongoCollection.filter(col("year_month_day_hour") === clusteringYMDH && col("timestep") === clusteringTimestep).select("gis_join", "temp_surface_level_kelvin")


+--------+-------------------------+-------------------+
|gis_join|temp_surface_level_kelvin|           features|
+--------+-------------------------+-------------------+
|G4804230|        281.4640808105469|[281.4640808105469]|
|G5600390|        265.2140808105469|[265.2140808105469]|
|G1701150|        265.7140808105469|[265.7140808105469]|
|G0601030|        282.9640808105469|[282.9640808105469]|
|G3701230|        279.2140808105469|[279.2140808105469]|
|G3700690|        280.8390808105469|[280.8390808105469]|
|G3701070|        280.9640808105469|[280.9640808105469]|
|G4803630|        275.7140808105469|[275.7140808105469]|
|G5108200|        273.4640808105469|[273.4640808105469]|
|G4801170|        269.3390808105469|[269.3390808105469]|
+--------+-------------------------+-------------------+
val assembler: VectorAssembler = new VectorAssembler().setInputCols(clusteringFeatures).setOutputCol("features")
val withFeaturesAssembled: Dataset[Row] = assembler.transform(clusteringCollection)



+--------+--------------------+
|gis_join|            features|
+--------+--------------------+
|G4804230|[0.6709129511677282]|
|G5600390|[0.3949044585987261]|
|G1701150|[0.4033970276008492]|
|G0601030|[0.6963906581740976]|
|G3701230|[0.6326963906581741]|
|G3700690|[0.6602972399150743]|
|G3701070|[0.6624203821656051]|
|G4803630|[0.5732484076433121]|
|G5108200| [0.535031847133758]|
|G4801170|[0.46496815286624...|
+--------+--------------------+
val minMaxScaler: MinMaxScaler = new MinMaxScaler().setInputCol("features").setOutputCol("normalized_features")
val minMaxScalerModel: MinMaxScalerModel = minMaxScaler.fit(withFeaturesAssembled)
var normalizedFeatures: Dataset[Row] = minMaxScalerModel.transform(withFeaturesAssembled)
normalizedFeatures = normalizedFeatures.drop("features")
normalizedFeatures = normalizedFeatures.withColumnRenamed("normalized_features", "features").select("gis_join", "features")


[0.5168304366844847]
[0.3680625754467921]
[0.6467503082873386]
[0.21075872847369662]
[0.8369497523000703]
val kMeans: KMeans = new KMeans().setK(clusteringK).setSeed(1L)
val kMeansModel: KMeansModel = kMeans.fit(normalizedFeatures)
val centers: Array[Vector] = kMeansModel.clusterCenters


+--------+--------------------+----------+
|gis_join|            features|prediction|
+--------+--------------------+----------+
|G4804230|[0.6709129511677282]|         2|
|G5600390|[0.3949044585987261]|         1|
|G1701150|[0.4033970276008492]|         1|
|G0601030|[0.6963906581740976]|         2|
|G3701230|[0.6326963906581741]|         2|
|G3700690|[0.6602972399150743]|         2|
|G3701070|[0.6624203821656051]|         2|
|G4803630|[0.5732484076433121]|         0|
|G5108200| [0.535031847133758]|         0|
|G4801170|[0.46496815286624...|         0|
+--------+--------------------+----------+
val predictions: Dataset[Row] = kMeansModel.transform(normalizedFeatures)

val clusters: Dataset[Row] = predictions.select("gis_join", "prediction").groupBy(col("prediction")).agg(collect_list("gis_join"))