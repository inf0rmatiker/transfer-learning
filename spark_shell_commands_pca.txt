import spark.implicits._
import org.apache.spark.SparkConf
import org.apache.spark.ml.clustering.{KMeans, KMeansModel}
import org.apache.spark.ml.feature.{MinMaxScaler, MinMaxScalerModel, PCA, PCAModel, VectorAssembler}
import org.apache.spark.ml.linalg.Vectors
import org.apache.spark.sql.expressions.Window
import org.apache.spark.sql.{DataFrame, Dataset, Row, RowFactory}
import org.apache.spark.sql.functions.{col, collect_list, min, row_number, struct}
import org.apache.spark.ml.regression.LinearRegression
import com.mongodb.spark.MongoSpark
import org.apache.spark.ml.feature.VectorAssembler
import org.apache.spark.ml.linalg.Vector
import org.apache.spark.ml.linalg.{DenseMatrix, DenseVector}
import org.apache.spark.sql.{Dataset, Row, SparkSession}

spark.conf.set("mongodb.keep_alive_ms", "100000")

var mongoCollection: Dataset[Row] = MongoSpark.load(spark)

val pcaFeatures: Array[String] = Array("mean_sea_level_pressure_pascal",
      "surface_pressure_surface_level_pascal",
      "orography_surface_level_meters",
      "temp_surface_level_kelvin",
      "2_metre_temp_kelvin",
      "2_metre_dewpoint_temp_kelvin",
      "relative_humidity_percent",
      "10_metre_u_wind_component_meters_per_second",
      "10_metre_v_wind_component_meters_per_second",
      "total_precipitation_kg_per_squared_meter",
      "water_convection_precipitation_kg_per_squared_meter",
      "soil_temperature_kelvin",
      "pressure_pascal",
      "visibility_meters",
      "precipitable_water_kg_per_squared_meter",
      "total_cloud_cover_percent",
      "snow_depth_meters",
      "ice_cover_binary")

val assembler: VectorAssembler = new VectorAssembler().setInputCols(pcaFeatures).setOutputCol("features")
val withFeaturesAssembled: Dataset[Row] = assembler.transform(mongoCollection)
val minMaxScaler: MinMaxScaler = new MinMaxScaler().setInputCol("features").setOutputCol("normalized_features")
val minMaxScalerModel: MinMaxScalerModel = minMaxScaler.fit(withFeaturesAssembled)
var normalizedFeatures: Dataset[Row] = minMaxScalerModel.transform(withFeaturesAssembled)
normalizedFeatures = normalizedFeatures.drop("features").withColumnRenamed("normalized_features", "features").select("gis_join", "features")
val pca: PCAModel = new PCA().setInputCol("features").setOutputCol("pcaFeatures").setK(6).fit(normalizedFeatures)
val pc: DenseMatrix = pca.pc
val pcaDF: Dataset[Row] = pca.transform(normalizedFeatures).select("gis_join", "features", "pcaFeatures");

val nElements = 6

val testDF1: Dataset[Row] = pcaDF.select(($"gis_join" +: Range(0, nElements).map(idx => $"pcaFeatures"(idx) as "PC" + (idx + 2)):_*))

val testDF2: Dataset[Row] = pcaDF.select($"gis_join", $"pcaFeatures"(0) as "PCA0", $"pcaFeatures"(1) as "PCA1", $"pcaFeatures"(2) as "PCA2")

val testDF: Dataset[Row] = pcaDF.groupBy("gis_join").avg("pcaFeatures")

scala> pcaDF.show()
+--------------------+--------------------+
|            features|         pcaFeatures|
+--------------------+--------------------+
|[0.54405509418675...|[0.80492832393268...|
|[0.55296738910269...|[0.80857487657638...|
|[0.55276483694551...|[0.76926659597087...|
|[0.55377759773141...|[0.79674422064381...|
|[0.54628316791573...|[0.71710709966826...|
|[0.55448653028154...|[0.77397137809210...|
|[0.55600567146040...|[0.68385808112507...|
|[0.55772736479643...|[0.66660908921372...|
|[0.55499291067449...|[0.75565165269005...|
|[0.55468908243872...|[0.72406202099240...|
|[0.55701843224630...|[0.70259295830020...|
|[0.55894267773951...|[0.65973792208689...|
|[0.55752481263925...|[0.70411932843546...|
|[0.55600567146040...|[0.72890749027291...|
|[0.55549929106744...|[0.73739245072801...|
|[0.55134697184525...|[0.84536048028294...|
|[0.55033421105934...|[0.84497922527196...|
|[0.55235973263115...|[0.83180258697776...|
|[0.55073931537370...|[0.84546368981010...|
|[0.54911889811626...|[0.84514470930467...|
+--------------------+--------------------+

scala> pcaDF.count()
res6: Long = 61029720